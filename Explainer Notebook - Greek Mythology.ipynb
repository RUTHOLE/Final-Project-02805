{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\color{brown}{\\text{Social Graphs Explainer Notebook}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\color{brown}{\\text{1. Motivation}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  $\\color{brown}{\\text{1.1  Dataset}}$\n",
    "In this project the dataset includes several different kind of characters from Greek mythology. The was extracted from this [Fandom page](https://mythus.fandom.com/wiki/Category:Greek_mythology) which contains content of figures, deities(gods), creatures, heroes and people. \n",
    "\n",
    "The different figures, people, deities etc has their own node. Inbetween different characters, as for example the two gods Zeus and Hera, there exist a link from Hera's web page to Zeus' web page. \n",
    "\n",
    "Each character has their own text assigned to them. The texts contains everything from the family tree of that character to enemies and brave histories. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  $\\color{brown}{\\text{1.2 Why this Dataset?}}$\n",
    "Greek mythology is broadly known as an old, ancient and religious subject. After looking further into this mythology, we understand there is more to the greek gods and godesses than we first thougth. Therefore, we proudly represent our notebook with interesting data analysis and plots to show users of this page what greek mythology is really all about. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  $\\color{brown}{\\text{1.3 Goal for you as an end user}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Greek mythology consists of tales and stories which are very dramatic and contain partnerships, intrigues and large family trees. \n",
    "Greek mythology has been used by artists and alike to draw inspiration from throughout time\n",
    "and even today, Greek mythology is still relevant as it is used for inspiration for many books, movies and games.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\color{brown}{\\text{2. Basic Statistics}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\color{brown}{\\text{2.1 Data Cleaning and Processing}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here follows a tutorial for how the data was extracted from the [Fandom Page](https://mythus.fandom.com/wiki/Category:Greek_mythology). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Firstly, the different web pages for the different characteres were examined. By looking at the Fandom API the different pages for the different characters were downloaded. Afte examining the different species in greek mythology it was decided to look into deities(gods), creatures, figures, heroes, peoples, personifictaions, cyclopes, mortals, stubs and titans. \n",
    "- Secondly, there were made a dataframe containing all the different characters with their title, what speicies they are, consorts, children, enemies and links. There is a total of 451 nodes with 2537 links. The characters that did not have any links to their site or links to other characters' sites were removed from the dataframe. These changes were done on the stubs category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request as urllib2\n",
    "response = urllib2.urlopen(\"https://mythus.fandom.com/wiki/Category:Greek_mythology\")\n",
    "html = response.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DEITIES\n",
      "\n",
      "https://mythus.fandom.com/api.php?action=query&list=categorymembers&cmtitle=Category:Greek_deities&cmlimit=500&format=json\n",
      "\n",
      "CREATURES\n",
      "\n",
      "https://mythus.fandom.com/api.php?action=query&list=categorymembers&cmtitle=Category:Greek_creatures&cmlimit=500&format=json\n",
      "\n",
      "FIGURES\n",
      "\n",
      "https://mythus.fandom.com/api.php?action=query&list=categorymembers&cmtitle=Category:Greek_figures&cmlimit=500&format=json\n",
      "\n",
      "HEROES\n",
      "\n",
      "https://mythus.fandom.com/api.php?action=query&list=categorymembers&cmtitle=Category:Greek_heroes&cmlimit=500&format=json\n",
      "\n",
      "PEOPLES\n",
      "\n",
      "https://mythus.fandom.com/api.php?action=query&list=categorymembers&cmtitle=Category:Peoples_in_Greek_mythology&cmlimit=500&format=json\n",
      "\n",
      "PERSONIFICATIONS\n",
      "\n",
      "https://mythus.fandom.com/api.php?action=query&list=categorymembers&cmtitle=Category:Personifications_in_Greek_mythology&cmlimit=500&format=json\n",
      "\n",
      "CYCLOPES\n",
      "\n",
      "https://mythus.fandom.com/api.php?action=query&list=categorymembers&cmtitle=Category:Cyclopes&cmlimit=500&format=json\n",
      "\n",
      "MORTALS\n",
      "\n",
      "https://mythus.fandom.com/api.php?action=query&list=categorymembers&cmtitle=Category:Mortals_from_Greek_myths&cmlimit=500&format=json\n",
      "\n",
      "STUBS\n",
      "\n",
      "https://mythus.fandom.com/api.php?action=query&list=categorymembers&cmtitle=Category:Stubs&cmlimit=500&format=json\n",
      "\n",
      "TITANS\n",
      "\n",
      "https://mythus.fandom.com/api.php?action=query&list=categorymembers&cmtitle=Category:Titans&cmlimit=500&format=json\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print (\"DEITIES\")\n",
    "# https://zelda.fandom.com/api.php\n",
    "baseurl = \"https://mythus.fandom.com/api.php?\"\n",
    "# rvslots = \"rvslots=*\"\n",
    "action = \"action=query\"\n",
    "listt = \"list=categorymembers\"\n",
    "# content = \"prop=revisions&rvprop=content\"\n",
    "dataformat = \"format=json\"\n",
    "title = \"cmtitle=Category:Greek_deities\"\n",
    "limit = \"cmlimit=500\"\n",
    "import requests\n",
    "\n",
    "print()\n",
    "query = \"%s%s&%s&%s&%s&%s\" %(baseurl,action, listt, title, limit, dataformat)\n",
    "print(query)\n",
    "url = query\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "with open('deities.json', 'wb') as f:\n",
    "    f.write(r.content)\n",
    "    f.close()\n",
    "    \n",
    "\n",
    "print()\n",
    "print (\"CREATURES\")\n",
    "# https://zelda.fandom.com/api.php\n",
    "baseurl = \"https://mythus.fandom.com/api.php?\"\n",
    "# rvslots = \"rvslots=*\"\n",
    "action = \"action=query\"\n",
    "listt = \"list=categorymembers\"\n",
    "# content = \"prop=revisions&rvprop=content\"\n",
    "dataformat = \"format=json\"\n",
    "title = \"cmtitle=Category:Greek_creatures\"\n",
    "limit = \"cmlimit=500\"\n",
    "import requests\n",
    "\n",
    "print()\n",
    "query = \"%s%s&%s&%s&%s&%s\" %(baseurl,action, listt, title, limit, dataformat)\n",
    "print(query)\n",
    "url = query\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "with open('creatures.json', 'wb') as f:\n",
    "    f.write(r.content)\n",
    "    f.close()\n",
    "\n",
    "print()\n",
    "print (\"FIGURES\")\n",
    "# https://zelda.fandom.com/api.php\n",
    "baseurl = \"https://mythus.fandom.com/api.php?\"\n",
    "# rvslots = \"rvslots=*\"\n",
    "action = \"action=query\"\n",
    "listt = \"list=categorymembers\"\n",
    "# content = \"prop=revisions&rvprop=content\"\n",
    "dataformat = \"format=json\"\n",
    "title = \"cmtitle=Category:Greek_figures\"\n",
    "limit = \"cmlimit=500\"\n",
    "import requests\n",
    "\n",
    "print()\n",
    "query = \"%s%s&%s&%s&%s&%s\" %(baseurl,action, listt, title, limit, dataformat)\n",
    "print(query)\n",
    "url = query\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "with open('figures.json', 'wb') as f:\n",
    "    f.write(r.content)\n",
    "    f.close()\n",
    "\n",
    "    \n",
    "print()\n",
    "print (\"HEROES\")\n",
    "# https://zelda.fandom.com/api.php\n",
    "baseurl = \"https://mythus.fandom.com/api.php?\"\n",
    "# rvslots = \"rvslots=*\"\n",
    "action = \"action=query\"\n",
    "listt = \"list=categorymembers\"\n",
    "# content = \"prop=revisions&rvprop=content\"\n",
    "dataformat = \"format=json\"\n",
    "title = \"cmtitle=Category:Greek_heroes\"\n",
    "limit = \"cmlimit=500\"\n",
    "import requests\n",
    "\n",
    "print()\n",
    "query = \"%s%s&%s&%s&%s&%s\" %(baseurl,action, listt, title, limit, dataformat)\n",
    "print(query)\n",
    "url = query\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "with open('heroes.json', 'wb') as f:\n",
    "    f.write(r.content)\n",
    "    f.close()\n",
    "    \n",
    "# ITEMS SKIPPED\n",
    "# LOCATIONS SKIPPED\n",
    "\n",
    "print()\n",
    "print (\"PEOPLES\")\n",
    "# https://zelda.fandom.com/api.php\n",
    "baseurl = \"https://mythus.fandom.com/api.php?\"\n",
    "# rvslots = \"rvslots=*\"\n",
    "action = \"action=query\"\n",
    "listt = \"list=categorymembers\"\n",
    "# content = \"prop=revisions&rvprop=content\"\n",
    "dataformat = \"format=json\"\n",
    "title = \"cmtitle=Category:Peoples_in_Greek_mythology\"\n",
    "limit = \"cmlimit=500\"\n",
    "import requests\n",
    "\n",
    "print()\n",
    "query = \"%s%s&%s&%s&%s&%s\" %(baseurl,action, listt, title, limit, dataformat)\n",
    "print(query)\n",
    "url = query\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "with open('peoples.json', 'wb') as f:\n",
    "    f.write(r.content)\n",
    "    f.close()\n",
    "    \n",
    "\n",
    "print()\n",
    "print (\"PERSONIFICATIONS\")\n",
    "# https://zelda.fandom.com/api.php\n",
    "baseurl = \"https://mythus.fandom.com/api.php?\"\n",
    "# rvslots = \"rvslots=*\"\n",
    "action = \"action=query\"\n",
    "listt = \"list=categorymembers\"\n",
    "# content = \"prop=revisions&rvprop=content\"\n",
    "dataformat = \"format=json\"\n",
    "title = \"cmtitle=Category:Personifications_in_Greek_mythology\"\n",
    "limit = \"cmlimit=500\"\n",
    "print()\n",
    "query = \"%s%s&%s&%s&%s&%s\" %(baseurl,action, listt, title, limit, dataformat)\n",
    "print(query)\n",
    "url = query\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "with open('personifications.json', 'wb') as f:\n",
    "    f.write(r.content)\n",
    "    f.close()\n",
    "    \n",
    "\n",
    "print()\n",
    "print (\"CYCLOPES\")\n",
    "# https://zelda.fandom.com/api.php\n",
    "baseurl = \"https://mythus.fandom.com/api.php?\"\n",
    "# rvslots = \"rvslots=*\"\n",
    "action = \"action=query\"\n",
    "listt = \"list=categorymembers\"\n",
    "# content = \"prop=revisions&rvprop=content\"\n",
    "dataformat = \"format=json\"\n",
    "title = \"cmtitle=Category:Cyclopes\"\n",
    "limit = \"cmlimit=500\"\n",
    "print()\n",
    "query = \"%s%s&%s&%s&%s&%s\" %(baseurl,action, listt, title, limit, dataformat)\n",
    "print(query)\n",
    "url = query\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "with open('cyclopes.json', 'wb') as f:\n",
    "    f.write(r.content)\n",
    "    f.close()\n",
    "    \n",
    "\n",
    "print()\n",
    "print (\"MORTALS\")\n",
    "# https://zelda.fandom.com/api.php\n",
    "baseurl = \"https://mythus.fandom.com/api.php?\"\n",
    "# rvslots = \"rvslots=*\"\n",
    "action = \"action=query\"\n",
    "listt = \"list=categorymembers\"\n",
    "# content = \"prop=revisions&rvprop=content\"\n",
    "dataformat = \"format=json\"\n",
    "title = \"cmtitle=Category:Mortals_from_Greek_myths\"\n",
    "limit = \"cmlimit=500\"\n",
    "print()\n",
    "query = \"%s%s&%s&%s&%s&%s\" %(baseurl,action, listt, title, limit, dataformat)\n",
    "print(query)\n",
    "url = query\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "with open('mortals.json', 'wb') as f:\n",
    "    f.write(r.content)\n",
    "    f.close()\n",
    "\n",
    "print()\n",
    "print (\"STUBS\")\n",
    "# https://zelda.fandom.com/api.php\n",
    "baseurl = \"https://mythus.fandom.com/api.php?\"\n",
    "# rvslots = \"rvslots=*\"\n",
    "action = \"action=query\"\n",
    "listt = \"list=categorymembers\"\n",
    "# content = \"prop=revisions&rvprop=content\"\n",
    "dataformat = \"format=json\"\n",
    "title = \"cmtitle=Category:Stubs\"\n",
    "limit = \"cmlimit=500\"\n",
    "print()\n",
    "query = \"%s%s&%s&%s&%s&%s\" %(baseurl,action, listt, title, limit, dataformat)\n",
    "print(query)\n",
    "url = query\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "with open('stubs.json', 'wb') as f:\n",
    "    f.write(r.content)\n",
    "    f.close()\n",
    "    \n",
    "\n",
    "print()\n",
    "print (\"TITANS\")\n",
    "# https://zelda.fandom.com/api.php\n",
    "baseurl = \"https://mythus.fandom.com/api.php?\"\n",
    "# rvslots = \"rvslots=*\"\n",
    "action = \"action=query\"\n",
    "listt = \"list=categorymembers\"\n",
    "# content = \"prop=revisions&rvprop=content\"\n",
    "dataformat = \"format=json\"\n",
    "title = \"cmtitle=Category:Titans\"\n",
    "limit = \"cmlimit=500\"\n",
    "print()\n",
    "query = \"%s%s&%s&%s&%s&%s\" %(baseurl,action, listt, title, limit, dataformat)\n",
    "print(query)\n",
    "url = query\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "with open('titans.json', 'wb') as f:\n",
    "    f.write(r.content)\n",
    "    f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def js_r(filename: str):\n",
    "    with open(filename) as f_in:\n",
    "        return json.load(f_in)\n",
    "    \n",
    "filename = 'creatures.json'\n",
    "d = js_r(filename)\n",
    "creatures = d['query']['categorymembers']\n",
    "\n",
    "filename = 'cyclopes.json'\n",
    "d = js_r(filename)\n",
    "cyclopes = d['query']['categorymembers']\n",
    "\n",
    "filename = 'deities.json'\n",
    "d = js_r(filename)\n",
    "gods = d['query']['categorymembers']\n",
    "\n",
    "filename = 'figures.json'\n",
    "d = js_r(filename)\n",
    "figures = d['query']['categorymembers']\n",
    "\n",
    "filename = 'heroes.json'\n",
    "d = js_r(filename)\n",
    "heroes = d['query']['categorymembers']\n",
    "\n",
    "filename = 'mortals.json'\n",
    "d = js_r(filename)\n",
    "mortals = d['query']['categorymembers']\n",
    "\n",
    "filename = 'peoples.json'\n",
    "d = js_r(filename)\n",
    "peoples = d['query']['categorymembers']\n",
    "\n",
    "filename = 'personifications.json'\n",
    "d = js_r(filename)\n",
    "personifications = d['query']['categorymembers']\n",
    "\n",
    "filename = 'stubs.json'\n",
    "d = js_r(filename)\n",
    "stubs = d['query']['categorymembers']\n",
    "\n",
    "filename = 'titans.json'\n",
    "d = js_r(filename)\n",
    "titans = d['query']['categorymembers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print(\"PANDAS DATAFRAME\")\n",
    "print()\n",
    "import pandas as pd\n",
    "characters = pd.DataFrame(columns=['Name', 'Category'])\n",
    "\n",
    "for entry in creatures[1:]:\n",
    "    characters = characters.append({'Name': entry['title'], 'Category': 'Creature'}, ignore_index=True)\n",
    "for entry in cyclopes:\n",
    "    characters = characters.append({'Name': entry['title'], 'Category': 'Cyclope'}, ignore_index=True)\n",
    "for entry in gods:\n",
    "    characters = characters.append({'Name': entry['title'], 'Category': 'God'}, ignore_index=True)\n",
    "for entry in figures:\n",
    "    characters = characters.append({'Name': entry['title'], 'Category': 'Figure'}, ignore_index=True)\n",
    "for entry in heroes:\n",
    "    characters = characters.append({'Name': entry['title'], 'Category': 'Heroe'}, ignore_index=True)\n",
    "for entry in mortals:\n",
    "    characters = characters.append({'Name': entry['title'], 'Category': 'Mortal'}, ignore_index=True)\n",
    "for entry in peoples:\n",
    "    characters = characters.append({'Name': entry['title'], 'Category': 'People'}, ignore_index=True)\n",
    "for entry in personifications:\n",
    "    characters = characters.append({'Name': entry['title'], 'Category': 'Personification'}, ignore_index=True)\n",
    "#NOTE: Some characters belong to the stubs category (f ex Alcmene, mother of Heracles)\n",
    "# TODO look at list(cat) and extract the relevant ones\n",
    "for entry in stubs:\n",
    "    characters = characters.append({'Name': entry['title'], 'Category': 'Stub'}, ignore_index=True)    \n",
    "for entry in titans[1:]:\n",
    "    characters = characters.append({'Name': entry['title'], 'Category': 'Titan'}, ignore_index=True)\n",
    "characters['Name'] = characters['Name'].str.strip()\n",
    "print(\"Initial length of the dataframe\", len(characters))\n",
    "\n",
    "# cat = characters[characters.Name.str.contains(\"Category\")].Name\n",
    "# list(cat)\n",
    "\n",
    "characters = characters.drop(characters[characters.Name.str.contains(\"Category\")].index).reset_index(drop = True)\n",
    "# characters[characters.duplicated(subset=['Name'], keep=False)].sort_values(by='Name').tail(103)\n",
    "characters.drop_duplicates(subset=['Name'], inplace = True, ignore_index = True, keep = 'last') #keep='first' by default\n",
    "print(\"Without Category entries and duplicates: \", len(characters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing and saving all characters' text in a file named after their title. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unidecode\n",
    "import json \n",
    "import requests\n",
    "import urllib.parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseurl = \"https://mythus.fandom.com/api.php?\"\n",
    "rvslots = \"rvslots=*\"\n",
    "action = \"action=query\"\n",
    "content = \"prop=revisions&rvprop=content\"\n",
    "dataformat = \"format=json\"\n",
    "path_root = \"./characters/\"\n",
    "#characters = []\n",
    "def api_to_txt(x):\n",
    "    filename = x.Name\n",
    "    titlename = urllib.parse.quote(filename)\n",
    "    # print(filename)\n",
    "    title = \"titles=\" + titlename.replace(\" \", \"_\")\n",
    "    query = \"%s%s&%s&%s&%s&%s\" %(baseurl, action, rvslots, title, content, dataformat)\n",
    "    r = requests.get(query)\n",
    "    response = r.text\n",
    "    response = urllib2.urlopen(query).read()\n",
    "    response_string = response.decode('utf-8').encode().decode('unicode-escape')\n",
    "    # redirect = re.findall(r'\\#REDIRECT \\[\\[(.*?)\\]\\]', response_string)\n",
    "\n",
    "    path = path_root + filename + \".txt\"\n",
    "    is_character = re.findall(r'Greek', response_string)\n",
    "    if (is_character):\n",
    "        text_file = open(path, \"wb\")\n",
    "        text_file.write(response)\n",
    "        text_file.close()\n",
    "    # else:\n",
    "    #     print(path + \" was not added\")\n",
    "\n",
    "a = characters.apply(lambda x: api_to_txt(x), axis=1)\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "def remove_from_dataframe(filename):\n",
    "    path = \"./characters/\" + filename + \".txt\"\n",
    "    if os.path.isfile(path):\n",
    "        return 1\n",
    "    else:\n",
    "        # print(filename, \"does not exist\")\n",
    "        return 0\n",
    "        \n",
    "# characters.apply(lambda x: remove_from_dataframe(x), axis = 1)\n",
    "characters = characters[characters['Name'].apply(remove_from_dataframe) != 0]\n",
    "characters.reset_index(inplace = True, drop=True)\n",
    "characters.is_copy = None #For the copy warning\n",
    "characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Extracting species, family, consorts etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from os.path import exists\n",
    "\n",
    "def get_species(x):\n",
    "    node_filename = x.Name\n",
    "    with open('characters/' + node_filename + '.txt', 'r') as f:\n",
    "        node_description = f.read()\n",
    "    \n",
    "    no_match = np.nan\n",
    "            \n",
    "    pattern_species = r'Species \\= (.*?)\\\\n'\n",
    "    species_block = redirect = re.findall(pattern_species, node_description)\n",
    "    \n",
    "    if not species_block:\n",
    "        species_block = no_match\n",
    "        species = species_block\n",
    "        \n",
    "    else:\n",
    "        species_search = r'\\[\\[(.*?)\\]\\]'\n",
    "        species = re.findall(species_search, species_block[0])\n",
    "        species = [a.encode().decode('unicode-escape') for a in species]\n",
    "        if not species:\n",
    "            species = np.nan\n",
    "\n",
    "    return species\n",
    "\n",
    "# characters2 = characters.iloc[0:5]\n",
    "characters[\"Species\"] = characters.apply(lambda x: get_species(x), axis=1)\n",
    "# list(characters.Species.dropna().values)\n",
    "\n",
    "\n",
    "def get_siblings(x):\n",
    "    node_filename = x.Name\n",
    "#     print(node_filename)\n",
    "#     print('characters/' + node_filename + '.txt')\n",
    "    with open('characters/' + node_filename + '.txt', 'r') as f:\n",
    "        node_description = f.read()\n",
    "    \n",
    "    no_match = np.nan\n",
    "            \n",
    "    pattern_siblings = r'Siblings \\= (.*?)\\\\n'\n",
    "    siblings_block = redirect = re.findall(pattern_siblings, node_description)\n",
    "    \n",
    "    if not siblings_block:\n",
    "        siblings_block = no_match\n",
    "        siblings = siblings_block\n",
    "        \n",
    "    else:\n",
    "        siblings_search = r'\\[\\[(.*?)\\]\\]'\n",
    "        siblings_list = re.findall(siblings_search, siblings_block[0])\n",
    "        siblings = [a.encode().decode('unicode-escape') for a in siblings_list]\n",
    "        if not siblings:\n",
    "            siblings = np.nan\n",
    "\n",
    "    return siblings\n",
    "characters[\"Siblings\"] = characters.apply(lambda x: get_siblings(x), axis=1)\n",
    "# list(characters.Siblings.dropna().values)\n",
    "\n",
    "\n",
    "def get_consorts(x):\n",
    "    node_filename = x.Name\n",
    "#     print(node_filename)\n",
    "#     print('characters/' + node_filename + '.txt')\n",
    "    with open('characters/' + node_filename + '.txt', 'r') as f:\n",
    "        node_description = f.read()\n",
    "    \n",
    "    no_match = np.nan\n",
    "            \n",
    "    pattern_consorts = r'Consort \\= (.*?)\\\\n'\n",
    "    consorts_block = redirect = re.findall(pattern_consorts, node_description)\n",
    "    \n",
    "    if not consorts_block:\n",
    "        consorts_block = no_match\n",
    "        consorts = consorts_block\n",
    "        \n",
    "    else:\n",
    "        consorts_search = r'\\[\\[(.*?)\\]\\]'\n",
    "        consorts_list = re.findall(consorts_search, consorts_block[0])\n",
    "        consorts = [re.sub(\" *\\\\(.*\", \"\", l) for l in consorts_list]\n",
    "        consorts = [re.sub(\" *\\\\|.*\", \"\", l) for l in consorts]\n",
    "        consorts = [a.encode().decode('unicode-escape') for a in consorts]\n",
    "        if not consorts:\n",
    "            consorts = np.nan\n",
    "\n",
    "    return consorts\n",
    "\n",
    "characters[\"Consorts\"] = characters.apply(lambda x: get_consorts(x), axis=1)\n",
    "# list(characters.Consorts.dropna().values)\n",
    "# characters.Consorts.dropna()\n",
    "# TODO REBROWSE ZEUS\n",
    "\n",
    "\n",
    "def get_children(x):\n",
    "    node_filename = x.Name\n",
    "#     print(node_filename)\n",
    "#     print('characters/' + node_filename + '.txt')\n",
    "    with open('characters/' + node_filename + '.txt', 'r') as f:\n",
    "        node_description = f.read()\n",
    "    \n",
    "    no_match = np.nan\n",
    "            \n",
    "    pattern_children = r'Children \\= (.*?)\\\\n'\n",
    "    children_block = redirect = re.findall(pattern_children, node_description)\n",
    "    \n",
    "    if not children_block:\n",
    "        children_block = no_match\n",
    "        children = children_block\n",
    "        \n",
    "    else:\n",
    "        children_search = r'\\[\\[(.*?)\\]\\]'\n",
    "        children_list = re.findall(children_search, children_block[0])\n",
    "        children = [re.sub(\" *\\\\(.*\", \"\", l) for l in children_list]\n",
    "        children = [re.sub(\" *\\\\|.*\", \"\", l) for l in children]\n",
    "        children = [a.encode().decode('unicode-escape') for a in children]\n",
    "        if not children:\n",
    "            children = np.nan\n",
    "\n",
    "    return children\n",
    "\n",
    "characters[\"Children\"] = characters.apply(lambda x: get_children(x), axis=1)\n",
    "# list(characters.Children.dropna().values)\n",
    "# characters.Children.dropna()\n",
    "\n",
    "\n",
    "def get_enemies(x):\n",
    "    node_filename = x.Name\n",
    "    with open('characters/' + node_filename + '.txt', 'r') as f:\n",
    "        node_description = f.read()\n",
    "    \n",
    "    no_match = np.nan\n",
    "            \n",
    "    pattern_enemies = r'Enemies \\= (.*?)\\\\n'\n",
    "    enemies_block = redirect = re.findall(pattern_enemies, node_description)\n",
    "    \n",
    "    if not enemies_block:\n",
    "        enemies_block = no_match\n",
    "        enemies = enemies_block\n",
    "        \n",
    "    else:\n",
    "        enemies_search = r'\\[\\[(.*?)\\]\\]'\n",
    "        enemies_list = re.findall(enemies_search, enemies_block[0])\n",
    "        enemies = [re.sub(\" *\\\\(.*\", \"\", l) for l in enemies_list]\n",
    "        enemies = [re.sub(\" *\\\\|.*\", \"\", l) for l in enemies]\n",
    "        enemies = [a.encode().decode('unicode-escape') for a in enemies]\n",
    "        if not enemies:\n",
    "            enemies = np.nan\n",
    "\n",
    "    return enemies\n",
    "\n",
    "characters[\"Enemies\"] = characters.apply(lambda x: get_enemies(x), axis=1)\n",
    "# list(characters.Consorts.dropna().values)\n",
    "# characters.Consorts.dropna()\n",
    "\n",
    "at_least = characters.iloc[characters[['Species', 'Siblings', 'Consorts', 'Children']].dropna(thresh=1).index]\n",
    "\n",
    "# at_least.head(15)\n",
    "len(at_least)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links_characters(node_filename, links_list):\n",
    "    links_list = list(set(links_list))\n",
    "    characters_links_list = []\n",
    "    for i in range(len(links_list)):\n",
    "        if links_list[i] in characters.Name.values:\n",
    "            if not links_list[i] == node_filename:\n",
    "                characters_links_list.append(links_list[i])\n",
    "    return characters_links_list\n",
    "\n",
    "def get_links(node_filename):\n",
    "    with open('characters/' + node_filename + '.txt', 'r') as f:\n",
    "        node_description = f.read()\n",
    "\n",
    "    pattern = r'\\[\\[(.*?)\\]\\]'\n",
    "\n",
    "    find = re.findall(pattern, node_description)\n",
    "    all_links_list = find\n",
    "    characters_links_list = get_links_characters(node_filename, all_links_list)\n",
    "#     characters_links_list = all_links_list\n",
    "    return characters_links_list\n",
    "\n",
    "characters[\"Linked\"] = characters.apply(lambda x: get_links(x.Name), axis=1)\n",
    "characters.head(3)\n",
    "characters.Category.value_counts().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding number of nodes and links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def add_node_and_edges(net, node):\n",
    "    net.add_node(node.Name, Category = node.Category, Species = node.Species)\n",
    "    links = node.Linked\n",
    "    if links: \n",
    "        for link in links:\n",
    "            net.add_edge(node.Name, link)\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "net = nx.DiGraph()\n",
    "characters.apply(lambda x: add_node_and_edges(net, x), axis=1)\n",
    "print(\"Number of isolated nodes (degree zero): \", len(list(nx.isolates(net)))) \n",
    "\n",
    "net.remove_nodes_from(list(nx.isolates(net)))\n",
    "# plt.figure(3,figsize=(12,12)) \n",
    "# nx.draw(net, with_labels = True)\n",
    "# plt.show()\n",
    "# plt.savefig('characters_network.png')\n",
    "\n",
    "print(\"Number of nodes: \", len(net.nodes()))\n",
    "print(\"Number of links: \", len(net.edges()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A visualisation of the greek mythology dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "k = 200\n",
    "sampled_nodes = random.sample(net.nodes, k)\n",
    "sampled_graph = net.subgraph(sampled_nodes)\n",
    "\n",
    "def random_color():\n",
    "    r = lambda: random.randint(0,255)\n",
    "    return('#%02X%02X%02X' % (r(),r(),r()))\n",
    "\n",
    "categories = list(characters.Category.value_counts().keys())\n",
    "colors = [random_color() for i in range(len(categories))]\n",
    "\n",
    "node_color_zip = zip(categories, colors)\n",
    "node_color_map = dict(node_color_zip)\n",
    "node_colors =  [node_color_map[sampled_graph.nodes[node]['Category']] for node in sampled_graph]\n",
    "\n",
    "d = dict(nx.degree(sampled_graph))\n",
    "from matplotlib.pyplot import figure\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize = [20,15])\n",
    "#plt.title('Greek Mythology Network', size=30)\n",
    "nx.draw_kamada_kawai(sampled_graph, node_color = node_colors,  node_size=[v * 100 for v in d.values()], with_labels=True)\n",
    "plt.savefig('greek_network.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_color_map = node_color_map\n",
    "\n",
    "def edge_color_get(G, edge): \n",
    "    if (G.nodes[edge[0]]['Category']) == (G.nodes[edge[1]]['Category']):\n",
    "#         print('yes:', G_uni.nodes[edge[0]]['Role'], G_uni.nodes[edge[1]]['Role'])\n",
    "        return  edge_color_map[G.nodes[edge[0]]['Category']]\n",
    "    else:\n",
    "#         print('no:', G_uni.nodes[edge[0]]['Role'], G_uni.nodes[edge[1]]['Role'])\n",
    "        return '#000000'\n",
    "edge_colors = [edge_color_get(sampled_graph, edge) for edge in sampled_graph.edges()]\n",
    "\n",
    "fig = plt.figure(figsize = [20,15])\n",
    "nx.draw_kamada_kawai(sampled_graph, node_color = node_colors, edge_color = edge_colors, node_size=[v * 100 for v in d.values()], with_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fa2 import ForceAtlas2\n",
    "fig = plt.figure(figsize = [10,15])\n",
    "fa = ForceAtlas2(outboundAttractionDistribution=True,\n",
    "                 edgeWeightInfluence=5, gravity = 2000)\n",
    "positions = fa.forceatlas2_networkx_layout(sampled_graph, pos=None, iterations=500)\n",
    "\n",
    "import numpy as np\n",
    "nx.draw_networkx_edges(sampled_graph, pos=positions, edge_color = edge_colors)\n",
    "nx.draw_networkx_nodes(sampled_graph, pos=positions, node_color = node_colors,  node_size=[v * 100for v in d.values()])\n",
    "\n",
    "# labels_draw = ['Link', 'Calamity Ganon', 'Hylia']\n",
    "# labels = {}    \n",
    "# for node in G.nodes():\n",
    "#     if node in labels_draw:\n",
    "#         labels[node] = node\n",
    "# nx.draw_networkx_labels(sampled_graph, pos=positions, font_size = 12, labels = labels)\n",
    "nx.draw_networkx_labels(sampled_graph, pos=positions, font_size = 12)\n",
    "\n",
    "\n",
    "# plt.axis('off')\n",
    "plt.show()\n",
    "plt.savefig('FA.png')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fa2 import ForceAtlas2\n",
    "fig = plt.figure(figsize = [14,10])\n",
    "fa = ForceAtlas2(outboundAttractionDistribution=False,\n",
    "    linLogMode=False,\n",
    "    adjustSizes=False,\n",
    "    edgeWeightInfluence=1.0,\n",
    "    jitterTolerance=1.0,\n",
    "    barnesHutOptimize=True,\n",
    "    barnesHutTheta=1.2,\n",
    "    multiThreaded=False,\n",
    "    scalingRatio=2.0,\n",
    "    strongGravityMode=False,\n",
    "    gravity=1.0)\n",
    "\n",
    "\n",
    "positions = fa.forceatlas2_networkx_layout(sampled_graph, pos=None, iterations=200)\n",
    "import numpy as np\n",
    "nx.draw_networkx_edges(sampled_graph, pos=positions, edge_color = edge_colors)\n",
    "nx.draw_networkx_nodes(sampled_graph, pos=positions, node_color = node_colors,  node_size=[v * 100 for v in d.values()])\n",
    "# nx.draw_networkx_labels(sampled_graph, pos=positions, font_size = 12, labels = labels)\n",
    "nx.draw_networkx_labels(sampled_graph, pos=positions, font_size = 12)\n",
    "\n",
    "# plt.axis('off')\n",
    "plt.show()\n",
    "plt.savefig('FA2.png')\n",
    "# plt.show()\n",
    "print(\"Number of isolated nodes (degree zero): \", len(list(nx.isolates(sampled_graph))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The top connected character in greek mythology. i.e. find the node with the highest in-degree\n",
    "in_degree = sorted(net.in_degree, key=lambda x: x[1], reverse=True)\n",
    "in_degree[0]\n",
    "\n",
    "Print(\"This is not surprising since Zeus is the biggest name in Greek Mythology, which we already knew.. This means that other character pages mention him\")\n",
    "\n",
    "out_degree = sorted(net.out_degree, key=lambda x: x[1], reverse=True)\n",
    "out_degree[0]\n",
    "\n",
    "Print(\"Zeus also has the top out degree, which means that he mentions most peope in his page. \")\n",
    "\n",
    "#Top five connected characters in regards to their in degree\n",
    "for i in range(5):\n",
    "    print(in_degree[i])\n",
    "    \n",
    "#Top five connected characters in regards to their out degree\n",
    "for i in range(5):\n",
    "    print(out_degree[i])\n",
    "#Compute TC for each category (how many times a word appears in each characters category page files)\n",
    "\n",
    "#All categories\n",
    "cat_keys = characters.Category.value_counts().keys().tolist()\n",
    "cat_keys\n",
    "\n",
    "#Get all characters under each category\n",
    "mortals = at_least.loc[at_least['Category'] == 'Mortal','Name']\n",
    "peoples = at_least.loc[at_least['Category'] == 'People','Name']\n",
    "titans = at_least.loc[at_least['Category'] == 'Titan','Name']\n",
    "creatures = at_least.loc[at_least['Category'] == 'Creature','Name']\n",
    "gods = at_least.loc[at_least['Category'] == 'God','Name']\n",
    "figures = at_least.loc[at_least['Category'] == 'Figure','Name']\n",
    "personifications = at_least.loc[at_least['Category'] == 'Personification','Name']\n",
    "heroes = at_least.loc[at_least['Category'] == 'Heroe','Name']\n",
    "cyclopes = at_least.loc[at_least['Category'] == 'Cyclope','Name']\n",
    "gods\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  $\\color{brown}{\\text{2.2 Data Set Statistics}}$\n",
    "G_dir = sampled_graph\n",
    "\n",
    "fig, axs = plt.subplots(3, 2)\n",
    "fig.set_size_inches(20, 16)\n",
    "\n",
    "degrees = sorted(G_dir.degree(G_dir.nodes, weight=None), key=lambda x: x[1], reverse = True)\n",
    "degrees_dist = [tuples[1] for tuples in degrees] #always >0 because we already removed the isolated nodes\n",
    "hist_degree, bins =  np.histogram(degrees_dist, bins = 'auto')\n",
    "bins_plot = [(bins[i]+bins[i+1])/2 for i in range(len(bins)-1)]\n",
    "\n",
    "axs[0,0].hist(degrees_dist, bins='auto', color='#a56b8e', edgecolor ='#a56b8e')\n",
    "axs[0,0].set_title('Degree distribution of non-isolated nodes')\n",
    "axs[0,0].set_xlabel('In-degree')\n",
    "axs[0,0].set_ylabel('Count')\n",
    "axs[0,1].scatter(bins_plot,hist_degree, color = '#a56b8e')\n",
    "axs[0,1].set_yscale(\"log\")\n",
    "axs[0,1].set_xscale(\"log\")\n",
    "axs[0,1].set_title('Log-log degree distribution of non-isolated nodes')\n",
    "axs[0,1].set_xlabel('In-degree')\n",
    "axs[0,1].set_ylabel('Count')\n",
    "\n",
    "in_degrees = sorted(G_dir.in_degree(G_dir.nodes, weight=None), key=lambda x: x[1], reverse = True)\n",
    "in_degrees_dist = [tuples[1] for tuples in in_degrees]\n",
    "hist_in_degree, bins =  np.histogram(in_degrees_dist, bins = 'auto')\n",
    "bins_plot_in = [(bins[i]+bins[i+1])/2 for i in range(len(bins)-1)]\n",
    "\n",
    "axs[1,0].hist(in_degrees_dist, bins='auto', color='#93c47d', edgecolor ='#93c47d')\n",
    "axs[1,0].set_title('Incoming degree distribution of non-isolated nodes')\n",
    "axs[1,0].set_xlabel('Out-degree')\n",
    "axs[1,0].set_ylabel('Count')\n",
    "axs[1,1].scatter(bins_plot_in,hist_in_degree, color = '#93c47d')\n",
    "axs[1,1].set_yscale(\"log\")\n",
    "axs[1,1].set_xscale(\"log\")\n",
    "axs[1,1].set_title('Log-log incoming degree distribution of non-isolated nodes')\n",
    "axs[1,1].set_xlabel('Out-degree')\n",
    "axs[1,1].set_ylabel('Count')\n",
    "\n",
    "\n",
    "out_degrees = sorted(G_dir.out_degree(G_dir.nodes, weight=None), key=lambda x: x[1], reverse = True)\n",
    "out_degrees_dist = [tuples[1] for tuples in out_degrees]\n",
    "hist_out_degree, bins =  np.histogram(out_degrees_dist, bins = 'auto')\n",
    "bins_plot_out = [(bins[i]+bins[i+1])/2 for i in range(len(bins)-1)]\n",
    "\n",
    "axs[2,0].hist(out_degrees_dist, bins='auto', color='#5e4700', edgecolor ='#5e4700')\n",
    "axs[2,0].set_title('Outgoing degree distribution of non-isolated nodes')\n",
    "axs[2,0].set_xlabel('Degree')\n",
    "axs[2,0].set_ylabel('Count')\n",
    "axs[2,1].scatter(bins_plot_out,hist_out_degree, color = '#5e4700')\n",
    "axs[2,1].set_yscale(\"log\")\n",
    "axs[2,1].set_xscale(\"log\")\n",
    "axs[2,1].set_title('Log-log outgoing degree distribution of non-isolated nodes')\n",
    "axs[2,1].set_xlabel('Degree')\n",
    "axs[2,1].set_ylabel('Count')\n",
    "\n",
    "plt.savefig('dist.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import powerlaw\n",
    "\n",
    "def fit_plot_powerlaw(G, deg):\n",
    "    if deg == \"total\":\n",
    "        degrees = (list(dict(G.degree()).values()))\n",
    "    elif deg == \"in_degree\":\n",
    "        degrees = (list(dict(G.in_degree()).values()))\n",
    "    else:\n",
    "        degrees = (list(dict(G.out_degree()).values()))\n",
    "        \n",
    "    while 0 in degrees:\n",
    "        degrees.remove(0)\n",
    "    pfit = powerlaw.Fit(degrees, discrete=True)\n",
    "    print(\"alpha: \", pfit.alpha)\n",
    "    pfit.power_law.plot_pdf(color='g', label='Power law fit')\n",
    "    pfit.plot_pdf(marker='o', linewidth=0, linear_bins=True)\n",
    "    \n",
    "fit_plot_powerlaw(G_dir, deg=\"in_degree\")\n",
    "fit_plot_powerlaw(G_dir, deg=\"out_degree\")\n",
    "fit_plot_powerlaw(G_dir, deg=\"total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\color{brown}{\\text{3. Tools, theory and analysis}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  $\\color{brown}{\\text{3.1 Working with the text}}$\n",
    "To work with the characters text from the Fandom site it was necessary to use regular expressions to substract unnessecary words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  $\\color{brown}{\\text{3.2 Network science tools and data analysis strategies}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  $\\color{brown}{\\text{3.3 Understanding the Dataset}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  $\\color{brown}{\\text{3.4 Word Clouds}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import regex as re\n",
    "import os\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import regex as re\n",
    "import os\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from PIL import Image\n",
    "stopwords = STOPWORDS\n",
    "\n",
    "characters.Category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Patterns to remove stopwords, special signs and other \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removes_n = r\"\\\\n\"\n",
    "removes_intro = \"\\\"(.*?)\\\"\\:\"\n",
    "removes_categories = r\"\\[\\[category:\\w.*\\}\"\n",
    "removes_signs = \"[^\\w\\s]\"\n",
    "removes_numbers = \"\\d\"\n",
    "removes_titles =r\"\\\\n\\|\\w(.*?)\\=\"\n",
    "removes_trivials = r\"\\\\n\\\\n\\=\\=\\w(.*?)\\=\\=\\\\n\"\n",
    "removes_links = \"(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w \\.-]*)\"\n",
    "removes_latin = r\"latin\\=\\w(.*?)\\\\n\"\n",
    "removes_s = \"\\ss\\s\"\n",
    "removes_ufs = r\"\\\\u(?<=u)\\d...\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# iterate over all filens in the character folder to clean their text and save in character_clean\n",
    "filenames = []\n",
    "for file in os.listdir(\"./characters/\"):\n",
    "    filename = os.fsdecode(file)\n",
    "    if filename.endswith( ('.txt') ): # only txt files\n",
    "        filenames.append(filename)\n",
    "filenames.sort() # filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#without/with names\n",
    "removable_words = ['jpg', 'wikipedia', 'infobox', 'download', 'gallery','latin', 'figure', 'mortals','references', 'ref', 'god', 'creature','greek']\n",
    "save_path = './characters_clean/'\n",
    "for file in os.listdir(\"./characters/\"):\n",
    "    if os.path.isfile(file):\n",
    "        with open (\"./characters/\" + file) as f:\n",
    "            lines = f.readlines()\n",
    "            lines = ''.join(lines)\n",
    "            #for name in characters['Name']: #delete hashtag to remove names\n",
    "             #   lines = re.sub(name,'',lines)\n",
    "            lines = lines.lower()\n",
    "            lines = re.sub(removes_ufs, '', lines)\n",
    "            lines = re.sub(removes_categories, '', lines)\n",
    "            lines = re.sub(removes_titles, '', lines)\n",
    "            lines = re.sub(removes_trivials, '', lines)\n",
    "            lines = re.sub(removes_links, '', lines)\n",
    "            lines = re.sub(removes_latin, '', lines)\n",
    "            lines = re.sub(removes_intro, '', lines)\n",
    "            lines = re.sub(removes_n, '', lines)\n",
    "            lines = re.sub(removes_numbers, '', lines)\n",
    "            lines = re.sub(removes_signs, ' ', lines)\n",
    "            for word in removable_words:\n",
    "                lines = re.sub(word, '', lines)\n",
    "            lines = re.sub(removes_s, '', lines)\n",
    "            path = save_path + file\n",
    "            text_file = open(path, \"w\") #save the files in an other folder - characters_clean\n",
    "            text_file.write(lines)\n",
    "            text_file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Text file for Gods\")\n",
    "Gods = characters['Category'] == 'God'\n",
    "Gods = characters[Gods]\n",
    "Name_gods = Gods['Name']\n",
    "#Gods\n",
    "Name_gods\n",
    "path_god = \"./god/\"\n",
    "lines_gods = []\n",
    "for files in os.listdir(\"./characters_clean/\"):\n",
    "    for names in Name_gods:\n",
    "        if names + '.txt' == files:\n",
    "            with open (\"./characters_clean/\" + files) as f:\n",
    "                lines_gods.append(f.readlines())\n",
    "\n",
    "lines_gods = ''.join(str(elem) for elem in lines_gods)\n",
    "lines_gods = re.sub(removes_signs, '', lines_gods)\n",
    "path = path_god +\"gods_text.txt\"\n",
    "text_file = open(path, \"w\") #save the files in an other folder - characters_clean\n",
    "text_file.write(lines_gods)\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Text file for Creatures\")\n",
    "Creatures = characters['Category'] == 'Creature'\n",
    "Creatures = characters[Creatures]\n",
    "Name_creature = Creatures['Name']\n",
    "\n",
    "path_creature = \"./creature/\"\n",
    "lines_creatures = []\n",
    "for files in os.listdir(\"./characters_clean/\"):\n",
    "    for names in Name_creature:\n",
    "        if names + '.txt' == files:\n",
    "            with open (\"./characters_clean/\" + files) as f:\n",
    "                lines_creatures.append(f.readlines())\n",
    "\n",
    "lines_creatures = ''.join(str(elem) for elem in lines_creatures)\n",
    "lines_creatures = re.sub(removes_signs, '', lines_creatures)\n",
    "path = path_creature +\"creatures_text.txt\"\n",
    "text_file = open(path, \"w\") #save the files in an other folder - characters_clean\n",
    "text_file.write(lines_creatures)\n",
    "text_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Text file for People\")\n",
    "People = characters['Category'] == 'People'\n",
    "People = characters[People]\n",
    "Name_people = People['Name']\n",
    "\n",
    "path_people = \"./people/\"\n",
    "lines_people = []\n",
    "for files in os.listdir(\"./characters_clean/\"):\n",
    "    for names in Name_people:\n",
    "        if names + '.txt' == files:\n",
    "            with open (\"./characters_clean/\" + files) as f:\n",
    "                lines_people.append(f.readlines())\n",
    "\n",
    "lines_people = ''.join(str(elem) for elem in lines_people)\n",
    "lines_people = re.sub(removes_signs, '', lines_people)\n",
    "path = path_people +\"people_text.txt\"\n",
    "text_file = open(path, \"w\") #save the files in an other folder - characters_clean\n",
    "text_file.write(lines_people)\n",
    "text_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Text file for Mortal\")\n",
    "Mortal = characters['Category'] == 'Mortal'\n",
    "Mortal = characters[Mortal]\n",
    "Name_mortal = Mortal['Name']\n",
    "\n",
    "path_mortal = \"./mortal/\"\n",
    "lines_mortal = []\n",
    "for files in os.listdir(\"./characters_clean/\"):\n",
    "    for names in Name_mortal:\n",
    "        if names + '.txt' == files:\n",
    "            with open (\"./characters_clean/\" + files) as f:\n",
    "                lines_mortal.append(f.readlines())\n",
    "\n",
    "lines_mortal = ''.join(str(elem) for elem in lines_mortal)\n",
    "lines_mortal = re.sub(removes_signs, '', lines_mortal)\n",
    "path = path_mortal +\"mortal_text.txt\"\n",
    "text_file = open(path, \"w\") #save the files in an other folder - characters_clean\n",
    "text_file.write(lines_mortal)\n",
    "text_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Text file for Titans\")\n",
    "Titan = characters['Category'] == 'Titan'\n",
    "Titan = characters[Titan]\n",
    "Name_titan = Titan['Name']\n",
    "\n",
    "path_titan  = \"./titan/\"\n",
    "lines_titan = []\n",
    "for files in os.listdir(\"./characters_clean/\"):\n",
    "    for names in Name_titan:\n",
    "        if names + '.txt' == files:\n",
    "            with open (\"./characters_clean/\" + files) as f:\n",
    "                lines_titan.append(f.readlines())\n",
    "\n",
    "lines_titan = ''.join(str(elem) for elem in lines_titan)\n",
    "lines_titan = re.sub(removes_signs, '', lines_titan)\n",
    "path = path_titan +\"titan_text.txt\"\n",
    "text_file = open(path, \"w\") #save the files in an other folder - characters_clean\n",
    "text_file.write(lines_titan)\n",
    "text_file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Plotting the five different Word Clouds\")\n",
    "wc = WordCloud(\n",
    "    background_color = 'white',\n",
    "    stopwords = stopwords,\n",
    "    height = 1400,\n",
    "    width = 1000\n",
    ")\n",
    "all_lines = [lines_gods, lines_people,lines_creatures,lines_mortal, lines_titan]\n",
    "titles = [\"God's text\", \"People's text\", \"Creature's text\", \"Mortal's text\", \"Titan's text\"]\n",
    "for alle in all_lines:\n",
    "    if alle == lines_gods:\n",
    "        generate_img = wc.generate(alle)\n",
    "        plt.imshow(generate_img, color_func = \"brown\")\n",
    "        plt.title(titles[0])\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "    if alle == lines_people:\n",
    "        generate_img = wc.generate(alle)\n",
    "        plt.imshow(generate_img)\n",
    "        plt.title(titles[1])\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "    if alle == lines_creatures:\n",
    "        generate_img = wc.generate(alle)\n",
    "        plt.imshow(generate_img)\n",
    "        plt.title(titles[2])\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "    if alle == lines_mortal:\n",
    "        generate_img = wc.generate(alle)\n",
    "        plt.imshow(generate_img)\n",
    "        plt.title(titles[3])\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "    if alle == lines_titan:\n",
    "        generate_img = wc.generate(alle)\n",
    "        plt.imshow(generate_img)\n",
    "        plt.title(titles[4])\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The five different WordClouds provides a good overview of what the gods, people, creatures, mortal and titans have in their text files. The gods clearly have one word in common - son. The people text have many familiar expressions as son, family and father. This gives an indication that the peoples text include many family relation. The creature text includes words as horse, centaur and sphinx which is not surprising since the text most likely descirbe what kind of species the creatuers are. In the WordCloud for the mortals three words stand out - son, king and troy. The Trojan war is a big event in the greek mythology which reflects the texts of the mortals. The word \"titan\" is the most repeated word of the titans, which is not very surprising but gaia - the godess of earth may also seem as a very important godess for the titans. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  $\\color{brown}{\\text{3.5 Communities and TF-IDF}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find communities and compute their associated TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import community\n",
    "G_undir = G_dir.to_undirected()\n",
    "partition = community.best_partition(G_undir, randomize = False)\n",
    "communities = list(set(partition.values()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find with this partition that there are 41 communities in total. The communities range from 0 to 41."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\color{brown}{\\text{4. Discussion}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  $\\color{brown}{\\text{4.1 What went well?}}$\n",
    "- The extraction of web sites, cleaning the text files, making the communities all went well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  $\\color{brown}{\\text{4.2 What is still missing?}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  $\\color{brown}{\\text{4.3 What could be improved?}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\color{brown}{\\text{5. Contributions}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ana - Data analysis, data processing, sentiment analysis and webpage \n",
    "- Iris - webpage\n",
    "- Ruth - Webpage, Word Clouds, explainer notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
